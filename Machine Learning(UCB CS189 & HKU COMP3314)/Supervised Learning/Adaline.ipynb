{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b23f051",
   "metadata": {},
   "source": [
    "# ADAptive LInear NEuron -> Adaline\n",
    "\n",
    "Adaline is a **improvement** on Perceptron algorithm. It firstly introduce the **linear activate function(derivable)** and **gradient descent** into neuron model, lay the mathematical foundation for modern deep learning!\n",
    "\n",
    "There are 4 parts of Adaline:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Input\n",
    "        C[\"Input vector X\"]\n",
    "        A[\"Bias (x₀=1)\"]\n",
    "    end\n",
    "\n",
    "    subgraph \"Parameters(Weights)\"\n",
    "        D[\"Weight vector W\"]\n",
    "        B[\"Bias weight w₀\"]\n",
    "    end\n",
    "\n",
    "    subgraph Computation\n",
    "        E[\"Net input<br/>z = w₀ + W·X\"]\n",
    "        F[\"Activation function<br/>(e.g., linear)\"]\n",
    "        G[\"Threshold function<br/>(e.g., sign(z))\"]\n",
    "    end\n",
    "\n",
    "    subgraph Output\n",
    "        H[\"Classification result\"]\n",
    "    end\n",
    "\n",
    "    %% 前向传播（实线）\n",
    "    A --> B\n",
    "    C --> D\n",
    "    B --> E\n",
    "    D -->|dot product| E\n",
    "    E --> F\n",
    "    E --> G\n",
    "    G --> H\n",
    "\n",
    "    %% 反馈更新（虚线，表示训练时的误差反馈）\n",
    "    F -.->|Update w₀<br/>by error| B\n",
    "    F -.->|Update W<br/>by error| D\n",
    "\n",
    "    %% 样式优化（可选）\n",
    "    classDef forward fill:#e6f3ff,stroke:#333;\n",
    "    classDef feedback stroke-dasharray: 5 5,stroke:#999;\n",
    "    class A,B,C,D,E,F,G,H forward\n",
    "    class F,B,D feedback\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc6079",
   "metadata": {},
   "source": [
    "## 1. Model structure and Mathematical principles\n",
    "+ **Activate function**:\n",
    "    $$\n",
    "    \\phi(z) = z = \\vec{w}^T\\vec{x} + b\n",
    "    $$\n",
    "+ **Decision function**:\n",
    "  + Same as Perceptron\n",
    "    $$\n",
    "    \\hat{y}=\n",
    "    \\begin{cases}\n",
    "    +1 &if z\\ge 0\\\\\n",
    "    -1 &otherwise\n",
    "    \\end{cases}\n",
    "    $$\n",
    "+ !!!**Cost function**\n",
    "  + We use **Mean Square Error(MSE)** to measure the accuracy of each sample\n",
    "    + i.e. the mean of the sum of the square of each sample's prediction result:\n",
    "  + For each sample $(\\vec{x}^{(i)},\\ y^{(i)}),y^{(i)}\\in\\set{+1,-1}$, we define:\n",
    "    Squared Error for the i-th sample:\n",
    "    $$\n",
    "        l^{(i)} = \\frac{1}{2}(y^{(i)}-\\phi(z^{(i)}))^2\n",
    "    $$\n",
    "    Sum of Squared Errors, **SSE**\n",
    "    $$\n",
    "        J = \\sum_i l^{(i)} = \\frac{1}{2}\\sum_i(y^{(i)}-\\phi(z^{(i)}))^2\n",
    "    $$\n",
    "    Mean Squared Error, **MSE**\n",
    "    $$\n",
    "        \\mathcal{L} = \\frac{SSE}{2n} = \\frac{1}{n}\\sum_i(y^{(i)}-\\phi(z^{(i)}))^2\n",
    "    $$\n",
    "  + Note that the MSE is **derivable** and **convex**\n",
    "\n",
    "By those definition, next we introduce the most important step of Adaline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18075781",
   "metadata": {},
   "source": [
    "## 2. Parameter update\n",
    "+ Gradient calculate:\n",
    "    $$\n",
    "    \\frac{\\partial l^{(i)}}{\\partial w_j} = -(y^{(i)} - \\phi(z^{(i)}))x_j^{(i)}\\\\\n",
    "    $$\n",
    "+ So we just need to move the weight follow the gradient's negative direction. But there is a queation: We should update weights for each samples, or we collect a batch samples and update weights once? Here are three way of the samples choose:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6af148",
   "metadata": {},
   "source": [
    "### 2.1 Stochastic Gradient Descent\n",
    "We update weights for each samples. i.e.:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\vec{w}\\leftarrow \\vec{w} - \\eta\\cdot\\Delta_wl^{(i)}(w) = w + \\eta(y^{(i)} - \\phi(z^{(i)}))\\vec{x}^{(i)}\\\\\n",
    "&b\\leftarrow b + \\eta(y^{(i)} - \\phi(z^{(i)}))\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where $\\eta$ is the hyper-parameter **learning rate**\n",
    "\n",
    "+ Since SGD move the weight for each sample, so it may jump left and right on both sides of the bottom of the MSE function. But because of its move pattern, sometimes it can jump out of the local optimal.\n",
    "+ So SGD perform well at some data set(but now noone will use it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c2d44",
   "metadata": {},
   "source": [
    "### 2.2 Batch Gradient Descent\n",
    "We use the MSE as the objactive function to update parameters. i.e.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\vec{w}\\leftarrow \\vec{w} - \\eta\\cdot\\Delta_w\\mathcal{L}(w) = w + \\eta\\frac{1}{n}\\sum_i(y^{(i)}-\\phi(z^{(i)}))\\\\\n",
    "&b\\leftarrow b + \\eta\\frac{1}{n}(y^{(i)} - \\phi(z^{(i)}))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "+ Pros:\n",
    "  + It has the most precise direction of how gradient descent\n",
    "+ Cons:\n",
    "  + Unefficient, waste of computation source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410b311",
   "metadata": {},
   "source": [
    "### 2.3 Mini-BGD 小批量梯度下降\n",
    "We use a little set of $b(1<b<n)$ samples as a batch to compute descient, balance the accuracy and efficiency.\n",
    "$$\n",
    "\\mathcal{B}\\text{ is the set of this bitch of samples:}\\\\\n",
    "\\begin{aligned}\n",
    "&\\vec{w}\\leftarrow \\vec{w} - \\eta\\cdot\\Delta_w\\mathcal{L}_{\\mathcal{B}}(w) = w + \\eta\\frac{1}{b}\\sum_{i\\in \\mathcal{B}}(y^{(i)}-\\phi(z^{(i)}))\\\\\n",
    "&b\\leftarrow b + \\eta\\frac{1}{b}(y^{(i)} - \\phi(z^{(i)}))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "+ The choice of $b$:\n",
    "  + Empirical law: \n",
    "    + Begin with 32\n",
    "    + 32-64-128-256...\n",
    "+ The mainstream of modern deep learning(CNN/RNN/Transformer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
