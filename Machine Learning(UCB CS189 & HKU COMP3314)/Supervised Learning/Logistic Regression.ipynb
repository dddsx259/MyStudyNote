{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09004bc1",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5af6c",
   "metadata": {},
   "source": [
    "## 1. Definitions\n",
    "\n",
    "### 1,1. logit/linear output z\n",
    "$$\n",
    "z = \\vec{w}^T\\vec{x} + b\n",
    "$$\n",
    "\n",
    "### 1.2. Sigmoid function\n",
    "We map the linear output $z$ into section $(0,1)$, explain as probability:\n",
    "$$\n",
    "P(y=1\\mid \\vec{x}, \\vec{w}) = \\phi(z)=\\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "So that:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\phi(z)\\rightarrow 1, &\\text{when } z\\rightarrow +\\infty\\\\\n",
    "\\phi(z)\\rightarrow 0, &\\text{when } z\\rightarrow -\\infty\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "And we have:\n",
    "+ If $P(y=1\\mid \\vec{x}, \\vec{w}) > 0.5$, output $\\hat{y}=1$\n",
    "+ Otherwise, output $\\hat{y}=0$\n",
    "\n",
    "### 1.3. Cost function\n",
    "We consider use **Likehood** to describe the error:\n",
    "For each sample $(\\vec{x}^{(i)}, y^{(i)})$, we define:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L^{(i)}\n",
    "&= P(y\\mid \\vec{x}, \\vec{w})\\\\\n",
    "&= (\\hat{y}^{(i)})^{y^{(i)}}(1-\\hat{y}^{(i)})^{1-y^{(i)}}\\\\\n",
    "&=\n",
    "\\begin{cases}\n",
    "\\hat{y}^{(i)}, &if\\ y=1\\\\\n",
    "1-\\hat{y}^{(i)}, &otherwise\\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where $\\hat{y}^{(i)} = \\phi(z)$    \n",
    "To simplize the expression of total cost(instead of use product), we let $\\mathcal{L} = log(L)$ \n",
    "i.e.\n",
    "$$\n",
    "\\mathcal{L}^{(i)}= - \\big[y^{(i)}\\log(\\hat{y}^{(i)}) + (1-y^{(i)})\\log(1 - \\hat{y}^{(i)})\\big]\\\\\n",
    "$$\n",
    "The $\\mathcal{L}$ we called **Log Loss/Cross-Entropy Loss**\n",
    "\n",
    "Then we define the total cost:\n",
    "### 1.4. Total cost\n",
    "$$\n",
    "J(\\vec{w}, b ) = \\frac{1}{m}\\sum_{i=1}^m\\mathcal{L}^{(i)}\n",
    "$$\n",
    "Which is a convex function, obviously we can use some optimization algorithm to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31618e",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent\n",
    "\n",
    "Note that:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}\\\\\n",
    "&\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So we have:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}\\\\\n",
    "&b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where $\\alpha$ is **Learning Rate**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d42fd",
   "metadata": {},
   "source": [
    "### 3. Multi-class logistic regression\n",
    "We consider use **Softmax function(Multinomial Logistic Regression)** to solve it.\n",
    "\n",
    "For each calss $k$ we have a specific weight $\\vec{w}_k and basis $b_k$. So each class has a logit:\n",
    "$$\n",
    "z_k = \\vec{w}_k^T\\vec{X} + b_k\n",
    "$$\n",
    "\n",
    "Then we hace the probability distribution:\n",
    "$$\n",
    "P(y=k\\mid \\vec{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^Ke^{z_j}}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
