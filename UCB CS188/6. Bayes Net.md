# 1. Probability Rundown
We just list some formula, no explanation.
1. $$P(A, B, C) = P(C, B, A)$$
2. $$P(A_1, A_2, \dots, A_k) = P(A_1) P(A_2|A_1) \cdots P(A_k | A_1, \dots, A_{k-1})$$
3. **Marginalization** or **Summing out**     
   $$P(A)=\sum_b\sum_c\cdots P(A,B=b,C=c,\cdots)$$
4. **Conditional Probability**.   
   $$P(A|B) = \frac{P(A, B)}{P(B)} \quad \text{（前提是 } P(B) > 0 \text{）}$$
5. **Bayes' Rule**
   +  $$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$
6. **Mutual Independence**
   A and B are independent IFF :
    - $P(A, B) = P(A) P(B)$
    - $P(A|B) = P(A)$, provided $P(B)>0$
    - $P(B|A) = P(B)$, provided $P(A)>0$
7. A is independence of B given $C_1,C_2,\cdots$: $A \perp\!\!\!\perp B \mid C_1,C_2,\cdots$

# 2. Probabilistic Inference
We consider that our world isn't in a specific state, i.e. each possible state in our world has its own probability. More precisely, our model is a **joint distribution,** i.e., a table of probabilities that captures the likelihood of each possible outcome, also known as an complete assignment. As an example, consider the table below:

| Season | Temperature | Weather | Probability |
|--------|-------------|---------|-------------|
| summer | hot         | sun     | 0.30        |
| summer | hot         | rain    | 0.05        |
| summer | cold        | sun     | 0.10        |
| summer | cold        | rain    | 0.05        |
| winter | hot         | sun     | 0.10        |
| winter | hot         | rain    | 0.05        |
| winter | cold        | sun     | 0.15        |
| winter | cold        | rain    | 0.20        |

## 2.1 Inference by Enumeration, IBE
We can use IBE to compute each probability we want by a complete **joint Probability Distribution Function**.

There are three kinds of variables:
$$P(Q|E=e)$$
+ **Query Variables**: The variable that we want to know the joint probability distribution
+ **Evidence Variables**: The known observed variables
+ **Hidden Variables**: Exists in joint PDF, but is neither QV nor EV.

The process of IBE:
To compute $P(Q_1,\cdots, Q_m|e_1,\cdots, e_n)$.   
1. **Filter**: Only preserve the rows **matching the observed evidence value**.
2. **Marginalize**: Sum up values of the hidden variable.
3. **Normalize**: Divide each item by the sum, s.t. the total probability is $1$

# 3 Bayesian Network Representation
Though we can solve any probability problem by IBE, the joint PDF is really huge.
> If we have $n$ random variables, each with $d$ possible value. The joint PDF will contain $d^n$ rows!

So we try to use a new structure to represent it!

## 3.1 Bayes Network
Two key idea of Bayes Net:
1. Use **DAG(Directed acyclic graph)** to represent the dependency relationship between variables.
   1. Each node in DAG represents an **random variable**
   2. Each directed edge represents a **direcy dependency relationship**(may not causality)
   3. MUst be **acyclic** !
2. Use multiple small **CPT(Conditional probability table)** instead of a huge joint PDF.
   1. For each variable $X$, the CPT gives all its **conditional probability** given its parent nodes' value
   2. The CPT contains:
      1. The combinations of all the values of the parent variables.
      2. Value of $X$
      3. The corresponding conditional probability.

## 3.2 How to compute probability (by Bayes Net)
The core formula(**Chain decomposition**)
$$
P(X_1, X_2, \dots, X_n) = \prod_{i=1}^{n} P\big(X_i \mid \text{Parents}(X_i)\big)
$$

## 3.3 Structure of Bayes Nets
Two important dependency relationship of Bayes Net:
1. Each node is conditional independent of all its non-descendant nodes in the DAG, given all of its parents.
2. **Markov Blanket**
   1. Each node is conditionally independent of all nodes, given its **Markov Blanket**.
   2. The Markov Blanket includes 3 kinds of nodes:
      1. Parents
      2. Children
      3. The other parents of its children (Co-parents), also called **Spouses**

Now we can prove why we can decompose joint PDF into CPTs:
We want to prove:
$$
P(X_1,X_2,...X_n)=\prod P\big(X_i|Parents(X_i)\big)
$$
Where $(X_1,X_2,\dots X_n)$ are in topological order.

LHS can be represent as:
$$
P(X_1)\cdot P(X_2|X_1)\cdot P(X_3|X_2,X_1)\cdot\dots
$$

By the **local Markov property**, each $X_i$ is conditionally independent of its non-desendants given its parents. Since the variables are ordered topologically, all ancestors of $X_i$ appear before it and thus:
$$
\forall i, P(X_i|X_{i-1},\dots X_1) = P\big(X_i|Parents(X_i)\big)\\
$$
Substituting into the chain rule yields the desired factorization.  
Q.E.D


# 4. D-Separation
## 4.1 Basical structure
How to judge weather two variables are conditional independent? We can use **D_Separation** to judge them only by **the structure of Bayes Net**.
Consider the following 3 **canonical triples**:
1. Causal Chain 因果链
   1. Structure: $X\rightarrow Y\rightarrow Z$
      1. Not observe $Y$: X and Y are dependent(i.e. informtion transmitted along the chain). 
      2. Given $Y$: $X\perp\!\!\!\perp Z\mid Y$.       
i.e. The two endpoint independent IFF the middle node are observed.

2. Common cause/Fork 共同原因
   1. Structure: $X\leftarrow Y\rightarrow Z$
      1. Not observe $Y$: X and Y are dependent
      2. Given $Y$: $X\perp\!\!\!\perp Z\mid Y$.   
i.e. The children independent IFF the co-parent are observed. 

3. Common effect/Collider
   1. Structure: $X\rightarrow Y\leftarrow Z$
      1. Not observe $Y$: $X\perp\!\!\!\perp Z$
      2. Given $Y$(or any of its descendants): $X$ and $Z$ **may** be dependent

## 4.2 The general rule of D-separation 
We want to judje weather $X$ and $Y$ are conditional independent given evidence set $\mathbb{Z}=\{Z_1,Z_2,\dots Z_k\}$.   
Precess:
1. Mark all evidence variables as **shaded**
2. Find all undirected path from $X$ to $Y$
3. For all path, check if it's **blocked**
   1. We divided each path into continuous triples.
   2. The path is blocked IFF there exist at least one blocked triple
4. If all path are blocked, then $X\perp\!\!\!\perp Y\mid\mathbb{Z}$


| Structure | middle node $Y$  | active or blocked？ |
|---|---|---|
| **Causal chain** ($X\leftarrow Y\rightarrow Z$) or **Common cause** ($X\leftarrow Y\rightarrow Z$) | not observed| active |
| **Causal chain** ($X\leftarrow Y\rightarrow Z$) or **Common cause** ($X\leftarrow Y\rightarrow Z$) | observed    | blocked |
| **Common Effect**($X\rightarrow Y\leftarrow Z$) | not observed | blocked |
| **Common Effect**($X\rightarrow Y\leftarrow Z$) | observed（or its descendent is observed） | active |