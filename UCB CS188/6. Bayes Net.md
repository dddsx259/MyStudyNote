# 1. Probability Rundown
We just list some formula, no explanation.
1. $$P(A, B, C) = P(C, B, A)$$
2. $$P(A_1, A_2, \dots, A_k) = P(A_1) P(A_2|A_1) \cdots P(A_k | A_1, \dots, A_{k-1})$$
3. **Marginalization** or **Summing out**     
   $$P(A)=\sum_b\sum_c\cdots P(A,B=b,C=c,\cdots)$$
4. **Conditional Probability**.   
   $$P(A|B) = \frac{P(A, B)}{P(B)} \quad \text{（前提是 } P(B) > 0 \text{）}$$
5. **Bayes' Rule**
   1. $$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$
6. **Mutually Independence**
   A is independence of B IFF :
    - $P(A, B) = P(A) P(B)$
    - $P(A|B) = P(A)$
    - $P(B|A) = P(B)$
7. A is independence of B given $C_1,C_2,\cdots$: $A \perp\!\!\!\perp B \mid C_1,C_2,\cdots$

# 2. Probabilistic Inference
We consider that our world isn's in a specific state, i.e. each possible state in our world has its own probability. More precisely, our model is a **joint distribution,** i.e., a table of probabilities that captures the likelihood of each possible outcome, also known as an assignment of variables. As an example, consider the table below:

| Season | Temperature | Weather | Probability |
|--------|-------------|---------|-------------|
| summer | hot         | sun     | 0.30        |
| summer | hot         | rain    | 0.05        |
| summer | cold        | sun     | 0.10        |
| summer | cold        | rain    | 0.05        |
| winter | hot         | sun     | 0.10        |
| winter | hot         | rain    | 0.05        |
| winter | cold        | sun     | 0.15        |
| winter | cold        | rain    | 0.20        |

## 2.1 Inference by Enumeration, IBE
We can use IBE to compute each probability we want by a complete **joint Probability Distribution Function**.

There are three kindy of variables:
$$P(Q|E=e)$$
+ **Query Variables**: The variable  thatwe want to know its probability distribution
+ **Evidence Variables**: The known observed variable
+ **Hidden Variables**: Exists in joint PDF, but is nother QV nor EV.

The process of IBE:
To compute $P(Q_1,\cdots, Q_m|e_1,\cdots, e_n)$.   
1. **Fliter**: Only perserve the rows with **same value as the observation**
2. **Marginalize**: Sum up all possible value of hidden variable.
3. **Normalize**: Devide each item by the sum, s.t. the total probability is $1$

# 3 Bayesian Network Representation
Though we can solve any probability problem by IBE, the joint PDF is really huge.
> If we have $n$ rendom variables, each with $d$ passible value. The joint PDF will contain $d^n$ rows!

So we try to use a new structure to represent it!

## 3.1 Bayes Network
Two key idea of Bayes Net:
1. Use **DAG(Directed acyclic graph)** to represent the dependency relationship between variables.
   1. Each node in DAG represents an **random variable**
   2. Each directed edge represents a **direcy dependency relationship**(may not causality)
   3. MUst be **acyclic** !
2. Use multiple small **CPT(Conditional probability table)** instead of a huge joint PDF.
   1. For each variable $X$, the COT gives all its **conditional probability** given its parent nodes' value
   2. The CPT contains:
      1. The combinations of all the values of the parent variables.
      2. Value of $X$
      3. The corresponding conditional probability.

## 3.2 How to compute probability(by Bayes Net)
The core formula(**Chain decomposition**)
$$
P(X_1, X_2, \dots, X_n) = \prod_{i=1}^{n} P\big(X_i \mid \text{Parents}(X_i)\big)
$$

## 3.3 Structure of Bayes Nets
Two important dependency relationship of Bayes Net:
1. Each node is conditional indenpendent of all its non-descendant nodes in the DAG, given all of its parents.
2. **Markov Blanket**
   1. Each node is conditional independent of all nodes, given its **Markov Blanket**.
   2. The Markov Blanket includes 3 kinds of nodes:
      1. Parents
      2. Children
      3. The other parents of its children(Co-parents), also called **Spouses**

Now we can prove why we can decompose joint PDF into CPTs:
We want to prove:
$$
P(X_1,X_2,...X_n)=\prod P\big(X_i|Parents(X_i)\big)
$$
Where $(X_1,X_2,\dots X_n)$ are in topological order.

LHS can be represent as:
$$
P(X_1)\cdot P(X_2|X_1)\cdot P(X_3|X_2,X_1)\cdot\dots
$$

Because of the first relationship, $X_i$ is conditional independent of all variables which isn't its parent, so:
$$
\forall i, P(X_i|X_{i-1},\dots X_1) = P\big(X_i|Parents(X_i)\big)\\
Q.E.D
$$

# 4. D-Separation