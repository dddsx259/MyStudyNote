在搜索问题中, 我们只需要考虑如何通过 **一般化的搜索算法** 找到目标态. 现在我们考虑我们的agent有一个或多个“敌人”, 这些敌人同样有自己的目标. 由于我们要考虑这些敌人的行为, 这不再是简单的搜索问题了. 我们考虑一类新的算法, 用于生成 **敌对搜索问题 adversarial search problem** 的解, 这类问题在ai领域也被称作 **游戏 Games**.

## 1. 游戏的分类
+ 从 **动作结果** 来看, 游戏可以是:
  + 确定性的 deterministic
  + 不确定性的 stochastic
+ 从 **玩家人数** 来分类
+ 从 **收益评估** 角度来看, 游戏可以是
  + 零和的 zero-sum
    + 一方的收益恒等于其他人的总亏损
  + 非零和的 non zero-sum

#### 确定性零和游戏 deterministic zero-sum game
我们首先考虑这类问题. 在这类问题中, 我们agent的每个action都是确定的, 并且我们获取的收益恒等于对手的损失.

对于这类问题最简单的思考方式就是用一个 **单一变量** 来描述这个游戏问题, 一方想让这个变量最大儿另一方想让他最小, 这有效的将所有“玩家”放入一个直接的竞争.

与我们前面学到的搜索问题, 算法返回一个“目标态/path”所不同, “敌对搜索算法“ 往往给出一个策略, 根据当前状态的信息, 推荐最优的下一步决策.

标准的game类型包含有下列定义:
+ 初始状态 $s_0$ 
+ 玩家,  $Players(s)$ 表示是谁的行动回合
+ 行动,  $Actions(s)$ 表示player的合法action
+ 转移模型 $Result(s,a)$ 
+ 终止测试 $Terminal\_test(s)$ 
+ 效用函数 $Utility(s,player)$ 

## 2. minimax algorithm
我们首先考虑确定性零和游戏问题中的 **极大极小算法 minimax**. 我们假设: 对手总是选择最有利的行动, 且对我们最不利. 我们考虑两个定义:
+ **状态价值state value**
  + 被定义为 **agent所能达到的最佳结果(也被称作效用 utility)**.
+ **终止效用 terminal utilities**
  + 即是终止状态的 **状态价值**.

我们先考虑仅有一个agent的情况:

#### 2.1 不考虑对抗
我们可以根据每个状态时能做出的 **行为** 以及 **转移模型**, 构建出一颗 **game tree**. 每个节点的后继子结点都是我们的agent能通过行动到达的状态.

而我们定义: 每个状态的 **状态价值** 都是其所有子代的状态价值的最大值, 从而我们想要找到最优解, 本质上就是沿着子代中state value最大的节点一路向下知道终止状态.

也即:
$$
\forall non-terminal\ states,V(s)=\max_{s'\in successors(s)}V(s')\\
\forall terminal\ states,V(s)\ is\ known\\
$$
对于所有**终止状态 (terminal states), **状态 $s$ 的价值 $V(s)$ 是预先已知的确定值** 。这是整个价值计算递归过程的基准情况（base case）
> 终止状态就是gameover

不难发现, minimax本质上就是DFS, 递归的获取所有状态的最大值.

我们考虑存在对手agent的情况.
#### 2.2 考虑对抗
存在对抗时, 我们的game tree就要考虑重新构建了. 我们假设一共有k个敌对agent存在, 显然场上一共有k+1个agent.
+ root: 根节点显然就是start state
+ 对于第 $n$ 层 (root算第 $0$ 层) 的节点, 其全部子结点是由该状态时, 第 $n\% (k+1) +1$ 个智能体所有可能的行动所能到达的状态. 
+ 也即第 $\forall n>0$ 层的每个状态, 都是经过第 $(n-1)\%(k+1)+1$ 个agent行动后抵达的状态
显然, 2.1中的game tree是 $k=0$ 的特例版本.

我们有:
$$
\begin{aligned}
&\forall non-terminal\ states, &&V(s)=\max_{s'\in successors(s)}V(s')\\
&\forall opponent-controlled\ states, &&V(s)=\min_{s'\in successors(s)}V(s')\\
&\forall terminal\ states, &&V(s)\ is\ known\\
\end{aligned}
$$

考虑如下伪代码(python风格)
```python
def value(state, agent):
  if the state is a terminial state:
    return state.utility
  elif the state.agent is MAX:
    return max_value(state, agent)
  else return min_value(state, agent)

def max_value(state, agent):
  v = -INF
  for each successor of state:
    v = max(v, value(successor))
  return v

def min_value(state, agent):
  v = INF
  for each successor of state:
    v = min(v, value(successor))
  return v
```

其中, 我们的智能体行动产生的state也被称作 **maximizer (state)**(也即寻求最大state value), 其他智能体的state被称作 **minimizer (state)** .

#### 2.3 Alpha-Beta Pruning
minimax算法十分优美而简洁, 但是其时间复杂度是指数级别的.我们假设 $b$ 是其分支因子,  $m$ 是其game 树深度, 那么时间复杂度近似为 $O(b^m)$ 级别.
> 对于国际象棋, 其分支因子 $b\approx 35$ , 树深度 $m\approx 100$ . 显然是很难计算的.

我们考虑通过某些策略进行剪枝

```python
def value(state, agent, a, b):
  if the state is a terminial state:
    return state.utility
  elif the state.agent is MAX:
    return max_value(state, agent, a, b)
  else return min_value(state, agent, a, b)

def max_value(state, agent, a, b):
  v = -INF
  for each successor of state:
    v = max(v, value(successor, a, b))
    if v >= b:
      return v
    a = max(a, v)
  return v

def min_value(state, agent, a, b):
  v = INF
  for each successor of state:
    v = min(v, value(successor, a, b))
    if v <= a:
      return v
    b = min(b, v)
  return v
```

考虑如下的game tree:
![img](https://img2024.cnblogs.com/blog/3668476/202511/3668476-20251130145045347-1985627398.png)
<!--![alt text](image-6.png)-->

其中倒三角表示minimizer, 正三角表示maximizer, 矩形表示terminal state.

我们注意到, 在我们计算完root的左一子结点为3之后, 继续去遍历中间的minimizer, 计算其左一子代为2, 此时我们发现, 我们没有必要去继续计算其剩下的子代. 因为这个minimizer的最大值不大于root的左一minimizer的3, 这也意味着 **其剩余的信息不影响root的取值** . 所以可以直接剪去.
![img](https://img2024.cnblogs.com/blog/3668476/202511/3668476-20251130145544064-2118869662.png)
<!--![alt text](image-7.png)-->

理论上, 这个算法可以把时间复杂度优化到 $O(b^{\frac{m}{2}})$ , 实际上通常也可以增加几层的 **可解层数**.


## 3. Evaluation Function

尽管Alpha-Beta剪枝可以帮助增加我们可以可行运行极小极大算法的深度，但这通常仍然远远不足以到达大多数游戏搜索树的底部。因此，我们转向**评估函数**——这些函数接收一个状态并输出该节点真实极小极大值的估计。

#### 目的与应用

通常，评估函数被简单地解释为为"更好"的状态分配比"更差"的状态更高的值。评估函数广泛应用于**深度限制的极小极大算法**中，我们将位于最大可解深度的非终止节点视为终止节点，使用精心选择的评估函数为它们分配模拟的终止效用。

由于评估函数只能产生非终止状态价值的估计值，这种方法消除了运行极小极大算法时最优玩法的保证。决策质量变得依赖于这些估计的准确性。

#### 设计考虑

在设计使用极小极大算法的智能体时，通常会投入大量思考和实验来选择评估函数。评估函数越好，智能体的行为就越接近最优。此外，在树中更深的位置使用评估函数往往能给出更好的结果——将评估函数的计算埋在博弈树更深处可以减轻对最优性的妥协。

评估函数在游戏中的作用与标准搜索问题中的启发式函数非常相似。

#### 常见设计模式

##### 线性组合方法

评估函数最常见的设计是特征的线性组合：

$$\text{Eval}(s) = w_1f_1(s) + w_2f_2(s) + ... + w_nf_n(s)$$

其中：
- 每个$f_i(s)$对应从输入状态$s$提取的一个特征
- 每个特征被分配一个相应的权重$w_i$
- 特征是从游戏状态中提取的数值元素

##### 示例：跳棋评估函数

在跳棋游戏中，我们可能构建一个具有这四个特征的评估函数：
- 智能体的兵数
- 智能体的王数
- 对手的兵数
- 对手的王数

然后，我们将根据它们的相对重要性选择适当的权重。例如：

$$\text{Eval}(s) = 2 \cdot \text{agent\_kings}(s) + \text{agent\_pawns}(s) - 2 \cdot \text{opponent\_kings}(s) - \text{opponent\_pawns}(s)$$

这反映了以下理解：
- 王比兵更有价值（权重2比1）
- 对手的棋子应该有负权重（零和环境）
- 我方棋子应该有正权重

##### 超越线性

评估函数设计可以相当自由，不一定必须遵循线性组合。例如：
- 基于神经网络的非线性评估函数在强化学习应用中很常见
- 决策树、支持向量机和其他机器学习模型可以作为评估函数
- 领域特定的启发式方法可能包含复杂的条件推理

最重要的标准是评估函数始终为更好的位置产生更高的分数。这通常需要对使用具有各种特征和权重的评估函数的智能体进行大量微调和实验

## 4. Expectmax
在maximax中, 我们假定敌人会做出对他们最优的决策(i.e.对我们最不利), 这种过于悲观的策略在实际中并不一定适用. 现实中很多游戏的对手可能是 **随机的** **非理性的**, 环境可能是 **不确定的**. 我们考虑引入 **机会结点 chance node**, 来推广maximum.

我们定义:
$$
\forall s\in \text{agent-controled states}, V(s) = \max_{s'\in successors(s)} V(s')\\
\forall s\in \text{chance node}, V(s) = \sum_{s'\in successors(s)} p(s'|s)V(s')\\
\forall s\in \text{terminal state}, V(s)\text{ is known}
$$ 
其中,  $p(s|s')$ 表示从状态 $s$ 通过某动作到状态 $s‘$ 的概率.
很显然, maximum算法本质上是将对敌人最不利的节点 $p(s|s')$ 分配为 $1$ , 其余分配为 $0$ .

注意, 在expectmax中, 我们无法使用 **$\alpha-\beta$ purning**, 因为每个子结点的值都影响着chance node的期望值.

#### Mixed layer types
在一些游戏中, 对手不止一个, 每个对手的行为方式也可能不同, 此时我们考虑建立多层相邻的chance nodes, 每层chance node遵循各自的策略, 从而适应多变的实际问题.

## 5. General Game 通用博弈
我们目前只讨论了 **Zero-Sum Game**, 也即对手的损失等于我们的获利. 但实际生活中很多agent之间的关系并非如此对立, 我们考虑换一种通用的方式来描述博弈:

#### Multi-Agent Utilities 多智能体效用
我们考虑用一个元组来建模General Game:
$$
(u_1,u_2,...u_n)
$$

其中 $u_i$ 表示第 $i$ 个agent的 **效用 utility**, 而每个agent只关注自己对应的效用

#### 示例：三智能体博弈树

考虑如下博弈树（原文中的图）：

- **红色节点**：由智能体 A 控制 → 它选择能**最大化红色效用**的子节点
- **绿色节点**：由智能体 B 控制 → 它选择能**最大化绿色效用**的子节点
- **蓝色节点**：由智能体 C 控制 → 它选择能**最大化蓝色效用**的子节点

每个叶节点是一个三元组，如 $ (5, 2, 5) $、$ (3, 4, 1) $ 等。

##### 决策过程（自底向上）：

1. 在**蓝色层**（C 的决策点）：  
   C 比较子节点的**第三个数字（蓝色效用）**，选最大的那个。

2. 在**绿色层**（B 的决策点）：  
   B 比较从子节点传上来的元组的**第二个数字（绿色效用）**，选最大的。

3. 在**红色层**（A 的决策点）：  
   A 比较子节点的**第一个数字（红色效用）**，选最大的。

最终，根节点的值是 **$ (5, 2, 5) $**。

---

#### 关键洞察：合作可能自发出现

虽然每个智能体**只自私地优化自己的效用**，但在某些结构下，**整体结果对所有参与者都还不错**。

> 这体现了“**通过计算涌现出的合作行为（cooperation through computation）**”。

例如，在上面的例子中：
- 智能体 A 本可以选择另一个分支得到更高的红色效用（比如 6），但那样可能导致 B 或 C 的效用极低（比如 0）
- 但由于博弈树的结构限制，A 最终选择的路径恰好也让 C 获得了高分（5），B 也得到了中等分数（2）

这说明：**即使没有显式的合作机制，合理的博弈结构也能引导出对多方有利的结果**。

---

#### 与零和博弈的对比

| 特性 | 零和博弈（Minimax/Expectimax） | 通用博弈（General Games） |
|------|-------------------------------|--------------------------|
| 效用表示 | 单一标量值（一方的收益 = 另一方的损失） | 多维效用元组（每个智能体独立） |
| 智能体目标 | 对抗：最小化对手效用 | 自利：最大化自身效用 |
| 是否合作 | 不可能（利益完全冲突） | 可能自发出现（利益部分一致） |
| 应用场景 | 国际象棋、围棋、扑克（近似零和） | 多机器人系统、交通调度、经济博弈 |

---

#### 总结

- **通用博弈**打破了“零和”的限制，允许每个智能体拥有**独立的效用函数**。
- 决策时，每个智能体在其控制的节点上**仅优化自己的效用分量**。
- 尽管没有强制合作，但**合理的博弈结构可能自然引导出共赢结果**。
- 这是迈向**真实世界多智能体系统**的重要一步——现实中的 AI 很少处于纯粹对抗环境中。


## 6. 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）

### 为什么需要 MCTS？

像围棋（Go）、国际象棋等游戏的状态空间极其庞大：
- 围棋的合法局面数量约为 $10^{170}$，远超宇宙原子总数
- 传统 **Minimax + α-β 剪枝** 在这种高分支因子（high branching factor）下完全不可行

于是，我们引入 **MCTS** —— 一种**基于随机模拟**（rollout）的**非穷举式搜索算法**，它不要求遍历整棵树，而是**智能地聚焦于最有希望的分支**。

> ✅ MCTS 是 AlphaGo 战胜人类围棋冠军的核心技术之一！

---

### MCTS 的两个核心思想

1. **通过随机模拟评估状态（Evaluation by Rollouts）**  
   - 从当前状态 $s$ 出发，用一个**简单策略**（如随机走子）一直玩到游戏结束
   - 重复多次，统计**胜率**（win rate）
   - 胜率 ≈ 该状态的价值（value）

2. **选择性搜索（Selective Search）**  
   - 不平均分配计算资源
   - **动态决定**：哪些分支值得多模拟？哪些可以少看甚至忽略？
   - 目标：在有限时间内，**最大化根节点决策的质量**

---

### 简单示例：三选一动作

假设当前状态有三个可选动作：**左、中、右**

#### 方法一：平均分配模拟（Naive）
- 每个动作模拟 100 次
- 结果：左（45% 胜）、中（30% 胜）、右（60% 胜）
- → 选“右”

但这样**浪费算力**！如果“中”前 10 次就只赢了 1 次，很可能很差，没必要再花 90 次验证。

#### 方法二：动态分配（MCTS 的思路）
- 初步模拟发现“中”表现差 → 把原本给“中”的 90 次模拟**重新分配给“左”和“右”**
- 这样能更精确地比较两个有希望的动作

#### 方法三：处理“高不确定性”
- 假设“左”模拟了 100 次，胜率 55%
- “右”只模拟了 10 次，胜率 60%
- 虽然“右”胜率更高，但**样本太少，估计不可靠**（方差大）
- → 应该再给“右”多几次模拟，确认它是否真的更好

> 🔑 关键问题：**如何平衡“利用已知好动作”和“探索不确定动作”**？

---

### 解决方案：UCB（Upper Confidence Bound）准则

MCTS 使用 **UCB1 公式** 来指导树的遍历，实现 **“探索-利用权衡”（Exploration vs Exploitation）**。

对任意节点 $n$，其 UCB 值为：

$$
\text{UCB1}(n) = 
\underbrace{
   \frac{U(n)}{N(n)}
   }_{\text{利用项（Exploitation）}} 
+ C \cdot \underbrace{\sqrt{\frac{\ln N(\text{parent}(n))}{N(n)}}}_{\text{探索项（Exploration）}}
$$

其中：
- $N(n)$：从节点 $n$ 出发进行的**总模拟次数**
- $U(n)$：**父节点所属玩家**在这些模拟中的**总获胜次数**
  - （注意：因为是零和博弈，只需记录一方胜负）
- $C$：用户设定的**探索权重参数**（通常取 $\sqrt{2}$ 或调优）
- $\ln N(\text{parent}(n))$：父节点的总访问次数的对数

#### 两项含义：
- **第一项**：胜率（$U/N$）→ 越高越“ promising”（有希望）
- **第二项**：访问次数越少，值越大 → 鼓励探索“未知”分支

> 🌟 **UCB 自动平衡**：  
> - 如果一个动作胜率高 → 第一项大 → 更可能被选（利用）  
> - 如果一个动作很少被试 → 第二项大 → 更可能被选（探索）

---

### MCTS 的四步循环（UCT 算法）

MCTS 通过重复以下四个步骤构建搜索树（常称 **UCT：UCB applied to Trees**）：

1. **选择（Selection）**  
   - 从根节点开始，**递归选择 UCB 值最大的子节点**，直到到达一个**未完全展开的节点**（即还有未尝试的动作）

2. **扩展（Expansion）**  
   - 为该节点**添加一个新子节点**（对应一个之前未模拟过的动作）

3. **模拟（Simulation / Rollout）**  
   - 从新节点开始，用**默认策略**（如随机走子）玩到游戏结束
   - 记录结果（赢/输）

4. **回溯（Backpropagation）**  
   - 将模拟结果（+1 赢 / 0 输）**沿路径反向更新**所有祖先节点的 $U$ 和 $N$

> 重复上述过程数千或数百万次（取决于时间限制）

---

### 最终决策

- 所有模拟结束后，**选择根节点下访问次数 $N$ 最多的子节点对应的动作**
  - 为什么不是选胜率最高的？  
    → 因为 UCB 机制保证：**真正好的动作会被访问更多次**（胜率高 + 探索充分）
- 随着模拟次数 $N \to \infty$，MCTS 的行为**收敛到 Minimax 最优策略**

---

### MCTS 的优势与局限

| 优点 | 局限 |
|------|------|
| ✅ 无需完整游戏模型（只需能模拟） | ❌ 需要大量模拟才能准确 |
| ✅ 天然支持并行计算 | ❌ 默认策略若太差，效率低 |
| ✅ 动态分配资源，聚焦关键分支 | ❌ 在某些“陷阱”局面可能误判（因随机 rollout 无法发现深层战术） |
| ✅ 适用于超大状态空间（如围棋） | |

> 💡 实际应用中（如 AlphaGo），MCTS 会结合**神经网络**来：
> - 替代随机 rollout（用策略网络预测走法）
> - 提供先验概率（引导 UCB 中的初始偏向）

---

### 总结

- **MCTS 是一种基于随机模拟 + 智能树搜索的决策算法**
- 它通过 **UCB 准则** 平衡 **探索**（exploit）
- 核心流程：**选择 → 扩展 → 模拟 → 回溯**
- 特别适合**分支因子大、难以评估状态价值**的游戏（如围棋、扑克）
- 是连接**经典搜索**与**现代深度强化学习**（如 AlphaZero）的重要桥梁