# 1. Reinforcement Learning

### 1.1 Feedback Loop

We called the tuple $(s,a,s',r) **sample**, where:
+ The agent in state $s$ at any timestep
+ Then it do an action $a$, result to a successor state $s'$
+ Get the reward $r$. 
  
Usually the agent will start from the **start state**, receive **samples** and finally stop at **terminal state**, We called the above process an **episode**.

There are mainly two types of reinforcement learning:
# 2. Model-based Learning
+ The agent try to estimate the **transition function** and the **reward function** of the environment.
+ Literal, the agent try to model the environment.

i.e. We want to estimate the two key function from the sample we get:
+ **Transition Function**: 
  + $\hat{T}(s,a,s') = P(s'|s,a)$
+ **reward function**: $\hat{R}(s,a,s')$

The foundation of model-based learning lives on **Large Number Theorey**
> i.e. When the number of sample is large enough, we can estimate probability by frequence.

We use **counting** to model the environment. When the agent in state $s$ do action $a$, and then obverse the transition to $s'$, then add 1 to the number of the tuple (s,a,s').

$$
\hat{T}(s,a,s') = \frac{count(s,a,s')}{\sum_{s''}(s,a,s'')}
$$

Note that we need a huge number of samples, so the efficience of exploration of the model-based learning is obviously slow.

# 3. Model-free learning
In model-fraa learning, the agent instead of trying to **structure the two function of the environment**, we try to learn policy by interacting with environment directly.

There are three core algorithms:
+ **passive RL** 被动学习
  + Learn to evaluate the reward of a fixed policy.
  1. **Directly Evaluation**
  2. **Timporal Difference Learning, TD**
+ **active RL** 主动学习
  + Q-Learning

## 3.1 Directly Evaluation
+ Fixed a polict $\pi$
+ Try many episodes follows $\pi$, receive the samples.
+ For each state $s$, record:
  + Visit times.
  + Total reward.
+ Then the **state value** is computed by:
$$
V^\pi(s) = \frac{\text{Total reward}}{\text{Visit times}}
$$

> i.e. the average reward of the state.


Obviously we note that, when we receive enough samples, the statew value will converge to the real state value, but it slow and waste the structure between states(transition function).

## 3.2 Temporal Difference Learning

The core idea of TD is **update the estimate value after each step**,by the following process.

When the agent in state $s$, perform action $a = \pi(s)$, transfer to $s'$, get reward $r$, we structure a sample:
$$
sample = r + \gamma V^\pi(s')
$$

Then we immediately update the state value, by **exponential moving average**;
$$
V^\pi(s)\leftarrow(1-\alpha)V^\pi(s)+\alpha\cdot sample = V^\pi(s) +\alpha(sample-V^\pi(s) )
$$

Where $\alpha\in[0,1]$ is **learning rate**.

### Learning Rate
+ We can initially assign $\alpha=1$
  + s.t. then first sample completely determine the initial value of state value.
+ As learning pregresses, **reduce the $\alpha$**(e.g. $\alpha_k=\frac{1}{k}$)
  + s.t. the weight of old samples **exponential damp**, because they based on the older and more inaccurate value estimates.

## 3.3 Q-Learning
What we really want is the optimal policy. If we know the Q-value, the we can get the optimal policy simply by function $\argmax_a(s)$. So we can directly learn Q-value instead of state value.

Same as TD, we have:
$$
(s,a,s',r)\\
sample = r + \gamma\max_{a'}Q(s',a')
$$

Then we imediately update the Q-value:
$$
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \cdot \text{sample} = Q(s, a) + \alpha \cdot \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

The expression within square brackets called **TD error**

### advantage
+ The QL indepentant on transition or reward function of environment
+ **off-policy**, enev we do **rendomly or bad** policy(to explore), we still can get optimal Q-value and the policy.

## 3.4 Approximate Q-Learning
Consider continuous problem, the state space is infinite. We can't compute the Q-value $Q(s,a)$ for all state. But we can use **function approximate** to represent Q-value for each state.

We try to use **feature vector** to express state:
$$
\vec{f}(s,a) = (f_1(s,a), f_2(s,a),...f_n(s,a))
$$
Then suppose the Q-value is the linear combination of those feature:
$$
Q(s,a) = \vec{w}\cdot\vec{f}(s,a) = \sum_{i=1}^nw_if_i(s,a)
$$
Then definite the TD error:
$$
\text{difference} = \left[ r + \gamma \max_{a'} Q(s', a') \right] - Q(s, a)
$$

Then update each weight $w_i$
$$
w_i \leftarrow w_i + \alpha \cdot \text{difference} \cdot f_i(s, a)
$$

# 4. Exploration and Utilization

In RL, the agent must tradeoff two aspects:
+ **Utilization**: Perform the currently considered optimal action, s.t. maximize the known reward.
+ **Exploration**: Try the action which we havn't already known completely, s.t. may find the better policy.

We have two ways to **Systematic balance those two aspects:

### 4.1 $\epsilon$-Greedy policy
The simplest and the most commonly used exploration policy.

+ We set a parameter $\epsilon\in[0,1]$.
+ $\forall s\in S$, we:
  + **Randomly choose an action(to explore)** with the probability $\epsilon$
  + Choose the action with **the biggest Q-value at present** with the probability $1-\epsilon$

#### problem and challenge
+ If the $\epsilon$ is too big(e.g. $\epsilon = 0.5$), then though we  already know the good policy, the agent still **waste performence** to explore other action.
+ If the $\epsilon$ is too small(e.g. $\epsilon = 0.01$), then the agent may **fall into a suboptimal policy**, slowly learn better policy.


### 4.2 Exploration Function
Instead of using probability to choose which action to explore, we use **Exploration function** to determine it.

We use a new updation of Q-value in Q-learning:
$$
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') \right](old)\\
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left[ r + \gamma \max_{a'} f(s', a') \right](new)
$$

The $f(s,a)$ is the **Ecploration function**:

$$
f(s,a) = Q(s,a) + \frac{k}{N(s,a)}
$$
Where $N(s,a)$ is the number of visits of $(s,a)$, $k$ is the parameter **Exploration strength**.