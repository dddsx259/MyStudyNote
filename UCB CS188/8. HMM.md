# 1. Markov Model

Markov model assume that each time's distribution only depends on **the previous one distribution**, which is known as **Markov property**. So obviously, to describe a Markov model, we have two core part:

## 1.1 Necessary Information of Markov model
1. **Initial distribution**: The pribability distribution of the 0-th day, i.e. $P(W_0)$
2. **Transition model**: Describe how distribution change to the next timestep's distribution, i.e. $P(W_{t+1}\mid W_t)$

By Markov property, we can simplize the joint probability:
+ If it isn't satisfies Markov property, the joint probability is:
  $$
  P(W_0,W_1,\dots W_n)
  =P(W_0)P(W_1\mid W_0)\dots P(W_n\mid W_0,,W_1,\dots W_{n-1})
  =\prod_{i=0}^nP(W_i\mid parents(W_i))\text{ (in Bayes Net)}
  $$
+ But if it satisfies, we know that $W_k$ is independent of $(W_{k-1},\dots W_0)$, so we have:
  $$
  P(W_0,W_1,\dots W_n)
  =P(W_0)P(W_1\mid W_0)\dots P(W_n\mid W_{n-1})
  =P(W_0)\cdot\prod_{i=0}^{n-1}P(W_{i+1}\mid W_i)
  $$

There is another important assumption:
### Stationarity 平稳性假设
We assume that the **transition model** holds over time, i.e.
$$
\forall t, P(W_{t+1}\mid W_t)\text{ is fixed}
$$

## 1.2 Mini-Forward Algorithm
> How to compute the distribution of time $t$ $P(W_t)$?
Though we can directly compute the huge joint PDF, we can use the Markov property to simplize it!
Note that:
$$
P(W_{i+1})=\sum_{w_i}P(W_{i+1}\mid w_i)P(w_i)
$$
i.e. we can recursive compute distribution from $0$ to the time we want.

## 1.3 Stationary Dirstribution

After passage of time, is there a **stationary distribution** so that the distribution remains the same?
i.e.
$$
\exist\pi, \forall i>\pi,\big(P(W_{i+1}) = P(W_i)\big)
$$

Obviously, it's equivalent to solve the equation:
$$
\begin{align}
&\sum_w P(w)=1\\
&\forall w, P(w) = \sum_{w'} P(w\mid w')P(w')
\end{align}
$$
The solution is the stationary distribution.

But depends on the initial distribution and the transition model, stationary may **not exist**, or **never can be achieve**!

# 2. Hiddden Markov Model

## 2.1 Definition
There are two variable in HMM:
+ **Hidden state**:
  + The state that we can't directly observe.
+ **Observation/Evidence**:
  + The evidence that we obtain. 
  + Thought there is a relationship between **hidden state** and **observation**, usually the mapping from hidden state to observation **will lose information**(e.g. multiple state correspond to a same observation)

A Hmm is completely defined by the following three parts:
+ Initial distribution $P(W_0)$
+ Transition model $P(W_{i+1}\mid W_i)$
  + Note that the transition model describe the relationship among the **hidden state**, not the observation.
+ Observation model $P(F_i\mid W_i)$

No we care about:
> After observe the first $i$ days observation $f_1,\dots f_i$, what is the hidden state $W_i$'s probability distribution on day $i$?

This distribution called **Belief**:
$$
B(W_i)=P(W_i\mid f_1,f_2,\dots f_i)
$$

And for convenience, we also define:
$$
B'(W_i)=P(W_i\mid f_1,f_2,\dots f_{i-1})
$$

## 2.3 Forward Algorithm
process:
1. Time Elapse Update
   1. Before know new observation, predict the next state.
   $$
   \begin{aligned}
   B'(W_{i+1})&=P(W_{i+1}\mid f_1,f_2,\dots f_{i-1})\\
   &=\sum_{w_i}P(W_{i+1}, w_i\mid f_1,f_2,\dots f_{i-1})\\
   &=\sum_{w_i}P(W_{i+1}\mid w_i, f_1,f_2,\dots f_{i-1})\cdot P(w_i\mid f_1,f_2,\dots f_{i-1})\\
   &=\sum_{w_i}P(W_{i+1}\mid w_i)\cdot B(w_i)
   \end{aligned}
   $$
   Then we consider the relationship between $B(W_i)$ and $B'(W_i)$
2. Observation Update
   $$
   \begin{aligned}
   B(W_{i+1})
   &=P(W_{i+1}\mid f_1,f_2,\dots f_i)\\
   &=\frac{P(W_{i+1},f_{i+1}\mid f_1,\dots f_i)}{P(f_{i+1}\mid f_1,\dots f_i)}\\
   &\propto P(W_{i+1},f_{i+1}\mid f_1,\dots f_i)\\
   &=P(f_{i+1}\mid W_{i+1},f_1,\dots f_i)P(W_{i+1}\mid f_1,\dots f_i)\\
   &=P(f_{i+1}\mid W_{i+1})B'(W_{i+1})
   \end{aligned}
   $$

3. Finally, normalize all pribability.

# 3. Viterbi Algorithm
We want to find the **most likely sequence** of hidden states the system followed, given the observed evidence variables so far.
i.e. 
$$
\argmax_{x_{1:N}}P(x_{a:N}\mid e_{1:N}) = \argmax_{x_{1:N}}P(x_{a:N},e_{1:N})
$$

Obviously we can traverse all hidden state's possible value to compute it, but it need $O(|V|^N)$ complexity. How to simplify it?

## 3.1 process of Viterbi Algorithm
This is a dynamic algorithm, that we record the path with the biggst probability to each state. There are two core array:
+ For each time $t$ and state $x_t$
  + $m_t[x_t]$: The biggest path probability until tuple $(t,x_t)$
  + $a_t[x_t]$: Record the last state of the optimal path to $x_i$

### Recursion Fomula:
For each timestep $t$ and state $x_t$
1. When $t=1$
   $$m_1[x_1] = P(x_1)\cdot P(e_1\mid x_1)$$
2. When $t>1$, 
   $$
   m_t[x_t] = P(e_t\mid x_t)\cdot \max_{x_{t-1}}P(x_t\mid x_{t-1})\cdot m_{t-1}[x_{t-1}]\\
   a_t[x_t]=\argmax_{x_{t-1}}P(x_t\mid x_{t-1})\cdot m_{t-1}[x_{t-1}]
   $$

### Process
1. Forward Pass
   1. From $t=1\ to\ N$, for each state $x_t$, compute $m_t[x_t],a_t[x_t]$
2. Find the most possible state in terminal time:
   $$
   x^*_N=\argmax_{x_N}m_N[x_N]
   $$
3. Backward Pass
   1. From $x^*_N$, use $a_t$ to find the whole optimal path $x^*_1,x^*_2,\dots x^*_N$