# MIT6.006 Lec3&5 sort
我们先详细的定义一下四种时间复杂度:
### 3.1.1 平均(期望)时间复杂度
即每个输入规模下所有可能输入对应用时的平均值的复杂度（**随机输入**下期望用时的复杂度）

### 3.1.2 最好时间复杂度
即每个输入规模下用时最短的输入对应的时间复杂度

### 3.1.3 最坏时间复杂度
即每个输入规模下用时最长的输入对应的时间复杂度

### 3.1.4 均摊复杂度 Amortized Analysis(来自[OI.wiki](https://oi.wiki/basic/amortized-analysis/))
均摊分析（Amortized Analysis）是一种用于分析算法和动态数据结构性能的技术。它不仅仅关注单次操作的成本，还通过评估一系列操作的**平均成本**，为整体性能提供更加准确的评估。均摊分析不涉及概率，且只能确保最坏情况性能的每次操作耗费的平均时间，并不能确认系统的平均性能。在最坏情况下，均摊分析通过将高成本操作的开销**分摊**到低成本操作上，确保整体操作的平均成本保持在合理范围内。

均摊分析通常采用三种主要分析方法：**聚合分析**、**记账分析**和**势能分析**。这些方法各有侧重，分别适用于不同的场景，但它们的共同目标是通过均衡操作成本，优化数据结构在最坏情况下的整体性能表现。
我们接下来以初始容量为$m=1$动态数组(dynamic array)的尾部插入操作为例, 分析三种分析方法下的均摊成本:
#### 3.1.4.1 聚合分析 (Aggregate Analysis)
聚合分析（Aggregate Analysis）通过计算一系列操作的总成本，并将其平均到每次操作上，从而得出每次操作的均摊时间复杂度。
对于动态数组的尾部插入, 有两种情况:
+ 若数组未满, 插入的成本为$O(1)$.
+ 若数组已满, 则插入操作需要先扩容, 需要$O(m)$的成本($m$为当前数组的大小), 然后进行$O(1)$的插入.
所以，为了计算$n$次插入操作的总成本，可以将其分开为两部分计算:
+ 插入成本: 每次插入都是$O(1)$, 故$n$次成本为$O(n)$
+ 扩容成本:
  + 每次扩容都发生在容量为$2$的整数次幂时(因为扩容操作为倍增数组大小), 故总时间成本为$1+2+2^2+...+2^k$, 其中$2^k$为不大于n的最大$2$的整数次幂.
  + 故总操作成本为: $1+2+2^2+...+2^k = \frac{2^{k+1}-1}{2-1} = O(n)$
+ 总均摊成本为: $\frac{O(n)+O(n)}{2}=O(1)$
+ 即使在最坏情况下，平均每次插入操作的成本依然是常数时间。

### 3.1.4.2 记账分析(Accounting Analysis)
记账法（Accounting Method）通过为每次操作预先分配一个固定的均摊成本来确保所有操作的总成本不超过这些预分配的成本总和。记账法类似于一种 费用前置支付 的机制，其中较低成本的操作会存储部分费用，以支付未来高成本的操作。
以动态数组的尾部插入为例:
1. 费用分配:
   1. 我们假设每次插入的成本为$1$, 那么我们记每次插入的均摊成本为$3$
      1. 其中, $1$为每次插入的成本为$1$
      2. 剩余得$2$是每次插入为未来的扩容操作"预存"的成本
      3. 即 $均摊成本=实际成本+预存成本$
2. 费用使用:
   1. 当数组已满时，需要进行扩容操作，实际成本为$O(m)$，其中$m$是当前数组的大小。
   2. 假设扩容前数组的元素数量为$n$，由于原数组的后半部分$\frac{n}{2}$个元素在插入时共预存了$n$单位的均摊成本，恰好足够支付扩容操作的成本。
   
### 3.1.4.3 势能分析(Potential Analysis)
势能分析（Potential Method）通过定义一个势能函数（通常表示为 $\Phi$），度量数据结构的**潜在能量**，即系统状态中的预留资源，这些资源可以用来支付未来的高成本操作。势能的变化用于平衡操作序列的总成本，从而确保整个算法的均摊成本在合理范围内。

1. 我们定义状态 $S$ 为某一时刻数据结构的**状态**，该状态可能包含元素数量、容量、指针等信息，其中定义初始状态为 $S_0$，即未进行任何操作时的状态。
2. 我们定义势能函数 $\Phi(S)$ 用于度量数据结构状态 $S$ 的势能，其满足以下两个性质:
   1. **初始势能**：在数据结构的初始状态$S_0$下，势能$\Phi(S_0) = 0$
   2. **非负性**：在任意状态$S$下，势能$\Phi(S) \geq 0$
3. 对于每个操作，其均摊成本 $\hat{c}$ 定义为:
$$ 
\hat{c} = c + \Phi(S') - \Phi(S)
$$
其中$c$为操作的实际成本，$S$ 和 $S'$ 分别表示操作前后的数据结构状态。
4. 该公式表明，均摊成本等于实际成本加上势能的变化。
   1. 如果操作增加了势能（即 $\Phi(S') > \Phi(S)$），则均摊成本上升；
   2. 如果操作消耗了势能（即 $\Phi(S') < \Phi(S)$），则均摊成本下降。

5. 我们可以通过势能函数来分析一系列操作的总成本。设 $S_1, S_2, \dots, S_m$ 为从初始状态 $S_0$ 开始，经过 $m$ 次操作后产生的状态序列，$c_i$ 为第 $i$ 次操作的实际开销，那么第 $i$ 次操作的均摊成本 $p_i$ 为：
$$
p_i = c_i + \Phi(S_i) - \Phi(S_{i-1})
$$
因此，$m$ 次操作的总时间花销为：
$$
\sum_{i=1}^m c_i = \sum_{i=1}^m p_i + \Phi(S_0) - \Phi(S_m)
$$
由于 $\Phi(S) \geq \Phi(S_0)$，总时间花销的上界为：
$$
\sum_{i=1}^m p_i \geq \sum_{i=1}^m c_i
$$
因此，若有 
$$
\forall i\in [1, m], p_i = O(T(n))
$$
则 $O(T(n))$ 是均摊复杂度的一个**上界**。

以动态数组的尾部插入为例:
定义如下的势能函数 $\Phi(h)$：
$$
\Phi(h) = 2n - m
$$
其中，$n$ 是数组中的元素数量，$m$ 是数组的当前容量。这个势能函数反映了数组中剩余可用空间的数量，即**当前容量和实际使用空间之间的差异**
每次插入有如下两种情况:
1. 空间未满, 无需扩容
   1. 操作成本：$O(1)$，因为只需插入一个元素
   2. 势能变化：插入后，元素数量增加 $1$，势能增加 $2$
      1. $\Phi(h') - \Phi(h) = (2(n + 1) - m) - (2n - m) = 2$
   3. 均摊成本: $n + 1 + (2 - n) = 3$
2. 空间已满($n=m$), 需要扩容
   1. 操作成本:
      1. 我们假定当前元素数$n=m$, 需要扩容至 $2n$
      2. 需要将当前所有元素复制至新数组并插入, 故实际操作成本为 $n+1$
   2. 势能变化: 
      1. 插入后, $m\rightarrow 2m,\ n\rightarrow n+1$
      2. 
      $$
      \begin{aligned}\Phi(h')-\Phi(h)
      &=(2(n+1)-2m)-(2n-m)\\
      &=2-m\\
      &=2-n\\
      \end{aligned}
      $$
   3. 均摊成本: $n+1 + (2-n) = 3
故所有均摊成本均为 $O(1)$

对于一个set interface, 我们想要按内在逻辑(intrinsic)寻找需要的key, 通常需要遍历一遍, 消耗 $O(n)$的时间, 那么如何加快这个操作? 最简单的方式就是在一个**有序数组**中查找key, 以下介绍几种为数组排序的方法:

> 每种排序算法可能有多种实现方法(包括在不同语言中也有不同的特性可以使用), 本文仅记载笔者认为更好的实现方法(均衡可理解性与高效)

## 3.2排序算法(sort algorithm)
+ 将array的元素排序成按key递增的形式
  + 更快的找到最大/最小值(第一个/最后一个元素)
  + 通过二分搜索(binary search)实现更快的查找, $\Theta(n)$时间
+ Input: (static) array A of n numbers 
+ Output: (static) array B(是A的一个有序排列sorted permutation)
  + permutation: 有相同元素不同顺序的array
  + sorted: $\forall i\in (0, n-2),\ b[i]<b[i+1]$
+ ![alt text](image-5.png)
以下是部分排序算法:
(在代码中, 未在行末用"#"标注时间复杂度的都是$O(1)$)

### 3.2.1 排列排序 Permutation Sort 
+ 若不考虑A中有相同元素, A有$n!$种排列, 我们枚举出每一种排列并验证其是否是有序的
  + (有点类似于让一只猴子敲打字机, 总有一种可能是能打出<<莎士比亚>>)
+ 完全不稳定的排序, 毕竟你运气好可以第一个排列就是正确的, O(n)解决问题
代码实现:
```python
def permutationSort(A):                     #T(n)
    for B in permutations(A):               #O(n!)
        if is_sorted(B):                    #O(n)
            return B                        

def permutations(A):                        #S(n)
    if len(A)<=1:
        yield A
    for i in range(len(A)):                 #O(n)
        rest = A[:i] + A[i+1:]
        for p in permutations(rest):        #S(n-1)
            yield [A[i]] + p
```
+ 时间复杂度:
  + 平均时间复杂度:
    + `is_sorted`: 每次验证的时间是$\Theta(n)$的, 因为需要遍历所有元素
    + `permutations`: 递归的生成A的所有排列, $S(n)=n*S(n-1)$, 且$S(1)=\Theta(1)$, 故$S(n)=\Theta(n!)$ 
    + 对于每次生成的排列, 都要进行一起验证, 故平均时间复杂度是$\Theta(n)\times\Theta(n!)=\Theta(n\cdot n!)$ 
  + 最好时间复杂度:
    + 当第一个便利的排列是有序时, 时间为验证所需时间$\Theta(n)$
  + 最坏时间复杂度:$\Theta(n\cdot n!)$ 

3.2.2插入排序和3.2.3选择排序是对少量项进行排序的常用排序算法，因为它们易于理解和实现。这两种算法都是逐步的(incremental)，因为它们维护并增长元素的的有序子集，直到所有项目都有序.
我们再补充两个评估(排序)算法的概念:

**就地**(in-place)与**稳定性**(stability)
+ "就地(in-place)":
  + 指排序算法在排序过程中只需要**常数级别**的额外空间(即 $O(1)$ 额外空间复杂度)
  + 通常这类算法在实现的过程中基本上是在输入的数据结构内部进行所有必要的交换和调整，而不需要大量的额外数组或其他数据结构来辅助排序
+ "稳定(stable)":
  + 一个排序算法是稳定的，如果它保持了相等元素的相对顺序
    + e.g. 如果排序前有`A[i]==A[j]`($i<j$), 那么排序后同样有`A[i]`的新索引小于`A[j]`的新索引
  + 这个特性对于某些需要保证相同值元素原有顺序的应用场景非常重要

### 3.2.2 选择排序 Selection Sort 
+ 每次遍历剩余序列找出其最大/最小值, 取出放在新序列尾/头. 然后继续递归遍历去除该最值的剩余序列.
  + 在MIT6.006中, 实现逻辑为每次遍历`A`的子序列`A[:i]`找到最大值, 并将最大值(前缀最大值prefix_max)与`A[i]`交换
+ 如果序列是基于**数组(array)/动态数组**实现的, 那么因为其在任意位置的插入/删除是$O(n)$级别, 所以使用swap导致其是不稳定的(比如本课中的代码实现)
+ 如果序列是基于**链表**实现的, 那么就是稳定的
![](selectionSort.gif)
+ 是"就地"的
```python
def selection_sort(A, i = None):                    #T(i)
  ’’’Sort A[:i + 1]’’’  
  if i is None: i = len(A)- 1                       
  if i > 0: 
    j = prefix_max(A, i)                            #S(i)
    A[i], A[j] = A[j], A[i] 
    selection_sort(A, i - 1)                        #T(i-1)

def prefix_max(A, i):                               #S(i)
  ’’’Return index of maximum in A[:i + 1]’’’ 
  if i > 0: 
    j = prefix_max(A, i - 1)                        #S(i-1)
    if A[i] < A[j]: 
     return j 
  return i
```
+ 时间复杂度:
  + 平均时间复杂度:
    + 对于`prefix_max`:
      + base case: $i=0$时, 最大值的索引即为0
      + 递推: 前缀最大值是`A[i]`与前$i-1$个元素最大值的最大值
      + $S(1)=\Theta(1),\ S(n)=S(n-1)+\Theta(1)=\Theta(n)$
    + 对于`selection_sort`:
      + base case: 只有一个元素时是有序的
      + 递推: 假设前面的递推是正确的, 那么i后元素均在正确的位置, 递推继续
      + $T(1)=\Theta(1),\ T(n)=T(n-1) + S(n) = \Theta(n^2)$
  + 最好/最坏时间复杂度:
    + 同样需要上述所有过程, 故是$\Theta(n^2)$

### 3.2.3 插入排序 Insertion Sort 
我们假定前i-1个元素已排列好, 即子序列`A[:i]` 是有序的, 那么我们只需要依此将`A[i]`从`A[i-1]`开始向前移动, 直至其到达合适的位置, 即`A[j]<=A[i]`.
![](insertionSort.gif)
+ 是稳定的
+ 是"就地"的
```python
def insertion_sort(A, i = None):                #T(i)
  ’’’Sort A[:i + 1]’’’ 
  if i is None: i = len(A)- 1 
  if i > 0: 
    insertion_sort(A, i - 1)                    #T(i-1)
    insert_last(A, i)                           #S(i)

def insert_last(A, i): 
  ’’’Sort A[:i + 1] assuming sorted A[:i]’’’ 
  if i > 0 and A[i] < A[i- 1]: 
    A[i], A[i - 1] = A[i - 1], A[i] 
    insert_last(A, i - 1)                       #S(i-1)
```
时间复杂度:
  + 平均时间复杂度:
    + 对于`insert_last`:
      + base case: i=0, 只有一个元素的序列是有序的
      + 递推: 假设`A[:i]`是有序的, 那么只需要将`A[i]`向前移动直到遇见比自己小的数为止, 此时`A[:i+1]`也是有序的, 递推成立
      + $S(1)=\Theta(1),\ S(n)=S(n-1)+\theta(1)=\Theta(n)$
    + 对于`insert_sort`:
      + base case: i=0, 只有一个元素的序列是有序的
      + 递推: 若`A[:i]`是有序的, 那么我们只需要继续操作下去即可
      + $T(1)=\Theta(1),\ T(n)=T(n-1) + S(n) = \Theta(n^2)$
  + 最好/最坏时间复杂度:
    + 同样需要上述所有过程, 故是$\Theta(n^2)$
  

### 3.2.4 归并排序 Merge Sort 
我们每次将序列分为长度大致相等的前后两半, 分别对其排序(递归的进行归并排序), 最后合并两个有序序列.
归并排序的平均时间复杂度是 $O(n\log n)$, 已经很接近线性时间复杂度了(比二次式接近了很多)
![](mergeSort.gif)
```python
def merge_sort(A, a = 0, b = None):                         #T(n)
  if b is None: 
    b = len(A) 
  if 1 < b- a: 
    c = (a + b + 1) // 2 
    merge_sort(A, a, c)                                     #T(n/2)
    merge_sort(A, c, b)                                     #T(n/2)
    L, R = A[a:c], A[c:b] 
    i, j = 0, 0 
    while a < b:                                            #O(n)用于merge
      if (j >= len(R)) or (i < len(L) and L[i] < R[j]): 
        A[a] = L[i] 
        i = i + 1 
      else: 
        A[a] = R[j] 
        j = j + 1 
        a = a + 1
```
+ 时间复杂度:
  + 平均时间复杂度:
    $$
    \begin{aligned}
    T(n)&=2T(\frac{n}{2}) + \Theta(n)\\
    &=\Theta(n\log^{0+1}n)\\
    &=\Theta(n\log n)
    \end{aligned}
    $$
+ 可以在实现时只使用 $O(1)$的额外空间, 但会极大的增加代码复杂程度, 故本实现还是非"就地"的
+ 是稳定的

通过3.2.4归并排序, 我们已经可以实现 $O(n\log n)$的排序,现在我们已经可以完成我们之前的表格了:
![alt text](image-7.png)
这种在构建容器时提前处理数据的方法, 我们称为预处理(preprocessing)

接下来是几种其他的常见排序方法

### 3.2.5 希尔排序 Shell Sort
是一种对插入排序的优化, 也称为缩小增量排序(Diminishing Increment Sort)。它通过将待排序的列表分成若干子列表，对每个子列表进行插入排序，逐步缩小子列表的间隔，最终完成对整个数组的排序
我们知道插入排序一般来说是低效的, 但是**对于几乎有序的数组是高效的**(接近$O(n)$)
希尔排序的**基本思想**是：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录"基本有序"时，再对全体记录进行依次直接插入排序。
**算法步骤:**
1. 选择增量序列：
   1. 选择一个增量序列（gap sequence），用于将列表分成若干子列表。常见的增量序列有**希尔增量**（$\frac{n}{2}, \frac{n}{4}, ..., 1$）等
2. 分组插入排序：
   1. 按照增量序列将列表分成若干子列表(如`[A[0], A[0+gap], A[0+2gap]...]` )，对每个子列表进行插入排序
3. 缩小增量：
   1. 逐步缩小增量，重复上述分组和排序过程，直到增量为 1
4. 最终排序：
   1. 当增量为 1 时，对整个列表进行一次插入排序，完成排序
![alt text](Sorting_shellsort_anim-1.gif)
> 初始状态：
列表：[8, 3, 1, 2, 7, 5, 6, 4]
初始增量：4（列表长度 8 的一半）
> 
> 第一轮（增量为 4）：
将列表分成 4 个子列表：
子列表 1：[8, 7]
子列表 2：[3, 5]
子列表 3：[1, 6]
子列表 4：[2, 4]
对每个子列表进行插入排序：
子列表 1：[7, 8]
子列表 2：[3, 5]
子列表 3：[1, 6]
子列表 4：[2, 4]
排序后的列表：[7, 3, 1, 2, 8, 5, 6, 4]
> 
> 第二轮（增量为 2）:
将列表分成 2 个子列表：
子列表 1：[7, 1, 8, 6]
子列表 2：[3, 2, 5, 4]
对每个子列表进行插入排序：
子列表 1：[1, 6, 7, 8]
子列表 2：[2, 3, 4, 5]
排序后的列表：[1, 2, 6, 3, 7, 4, 8, 5]
> 
> 第三轮（增量为 1）：
将整个列表视为一个子列表，进行插入排序
排序后的列表：[1, 2, 3, 4, 5, 6, 7, 8]   

```python
def shell_sort(A, gap=None):
  if(gap == None): gap = len(A)//2
  if(gap == 0): return
  for i in range(gap, len(A)):
    # 对每个子列表进行插入排序
    temp = A[i]  # 当前待插入元素
    j = i
    # 在子列表中找到合适的位置插入
    while j >= gap and A[j - gap] > temp:
      A[j] = A[j - gap]
      j -= gap
    A[j] = temp
  shell_sort(A, gap//2)
```
+ 时间复杂度:
  + 最优时间复杂度为 $O(n)$
  + 平均时间复杂度和最坏时间复杂度与间距序列的选取有关.总体来说在$O(n \log n)$ 到 $O(n²)$(最坏) 之间
    + 常见的增量序列:
      + 希尔增量：$\frac{n}{2}, \frac{n}{4}, ..., 1$，时间复杂度为 $O(n²)$。
      + Hibbard 增量：$1, 3, 7, 15, ..., 2^{k - 1}$，时间复杂度为 $O(n^\frac{3}{2})$
      + Sedgewick 增量：$1, 5, 19, 41, 109, ...$，时间复杂度为 $O(n^\frac{4}{3})$
    + 详见[OI.wiki-希尔排序](https://oi.wiki/basic/shell-sort/#%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6)
+ 是就地的
+ 不稳定排序

### 3.2.6 冒泡排序 Bubble Sort
每次从头开始依此检查相邻元素, 将较大的元素如冒泡般逐渐移至数组尾部, 最终直至数组有序
![](bubbleSort.gif)
递归实现:
```python
def bubble_sort(A, n=len(A)):        #T(n)
  if n == 1: return
  flag = True                        #若数组有序, 则一遍遍历便结束
  for i in range(n-1):
    if A[i] > A[i+1]:
      A[i], A[i+1] = A[i+1], A[i]
      flag = False
  if flag: return
  bubble_sort(A, n-1)               #T(n-1)
```
+ 时间复杂度:
  + 平均时间复杂度:
    + $T(n) = T(n-1) + \Theta(1) = \Theta(n^2)$
  + 最好时间复杂度:
    + 当数组本身有序时, 时间复杂度为遍历一遍数组的时间$\Theta(n)$
  + 最坏时间复杂度:
    + 最坏情况下, 需要进行$\frac{n(n-1)}{2}$次交换, 需要 $\Theta(n^2)$
+ 是稳定的(如果相同就不会交换)
+ 该实现下是"就地"的

基于比较的排序算法效率是有上限的, 因为基于排序作为属性划分只能获得常数级别的分支数量, 一个规模为 $n$ 的数组, 其拥有 $n!$ 规模的排列组合(也即其决策树有 $n!$ 个叶子), 故其决策树的最小高度为:
$$
\Omega(\log(n!))=\Omega(n\log n)
$$
(可由斯塔林公式证明)
故基于比较模型的排序算法, 其时间复杂度的下界为 $\Omega(n\log n)$, 上述有许多种算法可以达到.

但是当我们不再局限于只能进行比较时, 我们便能获得几种决策树分支大于常数级别, 也即时间复杂度可能优于 $\Omega(n\log n)$ 的排序算法.


(本课程中, 先提供了一种“Direct Access Array Sort”, 也即计数排序最直接而原始的版本, 只能排序key两两不等的数组, 故不详细介绍了, 而且计数排序也介绍了一种比前缀和方法更低效的使用链表储存重复元素的实现, 不加以介绍了)

### 3.2.7 计数排序 (Counting Sort)
使用一个额外的数组`B`, 其第 $i$ 项为待排序数组`A`中等于 $i$ 的元素的个数, 之后利用`B`排序`A`. 因为`B`的索引本身就是有序的, 而现代内存机制可以做到$O(1)$的随机访问, 所以相当于用很大的空间实现了排序
> 譬如`A=[1, 1, 4, 5, 1, 4]`, `B`的大小为6.
> 那么当`B`处理完后, `B=[0, 3, 0, 0, 2, 1]`
> 即意味着A中有3个1 ...
> 那么我们可以轻易写出A得到有序形式: 111445
> 
实现其逻辑很简单, 但不能保证其稳定性(即如何保证相同元素的顺序), 故我们加入前缀和的处理, 确保排序前后的稳定性
![](countingSort.gif)
```python
W = 114514 #最大元素
w = 0	#最小元素

def counting_sort(A):
	B = [0] * W
    result = [0] * len(A)
	for i in range(len(A)): B[A[i]] += 1
    for i in range(W-2, w-1, -1): B[i] += B[i+1]  #计算前缀和
    for i in range(len(A)):
    	result[B[A[i]]-1] = A[i]
        B[A[i]] -= 1
    return result
```
+ 时间复杂度:
	+ 平均, 最好, 最坏时间复杂度均为 $\Theta(n+W)$, 因为至少要遍历一遍A与B
	+ 采取了"空间换时间"的策略, 比如有10万个1-10000间的整数的排序, 这种算法可以近似达到$O(n)$的级别
+ 是稳定的(通过前缀和保证顺序)
+ 需要极大的额外空间(usually), 并非就地
+ 是一种非比较型的排序算法

### 3.2.8 元租排序 (Tuple Sort)

如果对于每个元素的内在**key**, 都有许多个关键字, 如`{x:(x.key1, x.key2,...x.keyn)}`, 那么按照**关键字重要性**排序. 有我们先对**重要性最小关键字**排序, 再对第二小关键字排序...以此类推, 每次排序都采用**static method**, 最终完成对所有元素的排序.

关于其详细的讨论, 我们在接下来的基数排序中进行讨论:

### 3.2.9 基数排序 (Radix Sort)
基数排序将待排序的元素拆分为 $n$ 个关键字，逐一对各个关键字排序后完成对所有元素的排序。

下面用 $a_i$ 表示元素 $a$ 的第 $i$ 关键字。

假如元素有 $n$ 个关键字，对于两个元素 $a$ 和 $b$，默认的比较方法是：

比较两个元素的第 $1$ 关键字 $a_1$ 和 $b_1$，如果 $a_1 < b_1$ 则 $a < b$，如果 $a_1 > b_1$ 则 $a > b$，如果 $a_1 = b_1$ 则进行下一步；
比较两个元素的第 $2$ 关键字 $a_2$ 和 $b_2$，如果 $a_2 < b_2$ 则 $a < b$，如果 $a_2 > b_2$ 则 $a > b$，如果 $a_2 = b_2$ 则进行下一步；
……
例子：
+ 如果对自然数进行比较，将自然数按个位对齐后往高位补齐 0，则一个数字从左往右数第 i 位数就可以作为第 i 关键字；
+ 如果对字符串基于字典序进行比较，一个字符串从左往右数第 i 个字符就可以作为第 i 关键字；
+ C++ 自带的 `std::pair` 与 `std::tuple` 的默认比较方法与上述的相同。

回到排序算法,
+ 如果是从第 $1$ 关键字到第 $n$ 关键字顺序进行比较，则该元组排序称为 **MSD（Most Significant Digit first）**
+ 如果是从第 $n$ 关键字到第 $1$ 关键字顺序进行比较，则该元组排序称为 **LSD（Least Significant Digit first）**

1. MSD 基数排序
基于我们自然的对元素比较大小的顺序, 我们先对所有元素的**第 $1$ 关键字**进行排序, 从而得到这些元素的**大致顺序**,然后在每组第 $1$ 关键字相同的元素中, 继续进行递归的基数排序...

考虑到桶排序同样是先将元素分组, 然后对每个桶进行独立的排序(通常是插入排序, 保证排序的稳定性), 这个排序同样可以是**桶排序**, 所以其实基于MSD的桶排序可以看作是**使用桶排序实现的桶排序**
也因此，可以提出 MSD 基数排序在时间常数上的一种优化方法：假如到某一步 桶的元素数量 $\le B$（B 是自己选的常数），则直接执行插入排序然后返回，降低递归次数。

一般而言，我们默认基数排序是稳定的，所以在 MSD 基数排序中，我们也仅仅考虑借助 **稳定算法**（通常使用计数排序）完成内层对关键字的排序。

故基于 MSD 的基数排序, 最本质上, 是用某种稳定排序方法实现按首要关键字顺序分配元素进桶的桶排序, 递归的使用该方法实现桶排序(好jb绕)
参考代码:
```python
num_of_key = 114514

def insert_sort_for_msd(A, n=0):
  if len(A)==1: return
  insert_sort_for_msd(A[:len(A)-1], n)
  for i in range(len(A)-1, 0, -1):
    if A[i][n] < A[i-1][n]:
      A[i], A[i-1] = A[i-1], A[i]
    else: return

def radix_msd_sort(A, n=0):
  if(n >= num_of_key):return
  if(len(A)<>=1):return
  insert_sort_for_msd(A, n)
  begin = 0
  end = 0
  key = A[0][n]
  for i in range(len(A)):
    end = i
    if(A[i][n]!=key):
      radix_msd_sort(A[begin:end], n+1)
      begin = end
      key = A[i][n]


example_data = {(1, 1, 4), (5, 1, 4), (1, 9, 1), (9, 8, 1), (0, 0, 0)}
“”“
  排序5个整数: 114, 514, 191, 981, 000
”“”
```

2. LSD 基数排序
